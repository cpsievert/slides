An Interactive Visualization Platform for Interpreting Topic Models
========================================================
author: Carson Sievert (joint work with Kenny Shirley)
date: 9-26-2013
transition: rotate
incremental: true

Watch the talk here: https://www.dropbox.com/s/datl8sshpp859sh/LDAviz.mov
Follow along here: http://cpsievert.github.com/slides/LDA/0926


The ugly truth
========================================================

<div align="center"><img src="statisticans.png" width=800 height=600/></div>

Overview
========================================================
type: prompt

1. What is a topic model?
2. Fitting topic models via Latent Dirichlet Allocation (LDA).
3. Fitting an LDA model with the `R` package `LDAviz`.
4. Interpreting model output.
  - Topic specific keywords.
  - 'Global' keywords.
5. Measuring similarity between topics.
6. Visualizing model output.

Example Doc
========================================================
type: prompt
title: false

<div align="center"><img src="abstract.png" /></div>

Latent Dirichlet Allocation (LDA)
========================================================

Given $T$ topics, the probability of the $i^{th}$ word $i \in \{1, \dots, V\}$ in a given document:

$p(w_i) = \sum_{j=1}^T p(w_i, z_j) = \sum_{j=1}^T p(w_i|z_j)p(z_j)$

* Let $\phi_{i, j} = p(w_i|z_j)$ be the distribution of words for a given topic. Let the resulting $V$ by $T$ matrix be $\Phi$.
* Let $\theta_{d, j} = p(z_j)$ be the distribution of topics (for a given document $d \in \{1, \dots, D\}$).
* Let the resulting $T$ by $D$ matrix be $\Theta$.
* Assign (you guessed it) Dirichlet priors to both $\Phi$ and $\Theta$.
* Sample a topic for each word (at every iteration) of a collapsed Gibbs sampler.

AP Case Study
========================================================

* A popular dataset in the LDA literature is a set of Associated Press articles. Here is an example of one article (or document).

```{r doc-term, echo=FALSE}
library(LDAviz)
data(APtopdocs, package="LDAviz")
APtopdocs[1,1]
```

Input structure
========================================================
left: 55%
incremental: false

```{r vec, echo=FALSE}
library(LDAviz)
data(APinput, package="LDAviz")
words <- APinput$vocab[APinput$word.id]
mat <- data.frame(words, doc_id=APinput$doc.id)
head(mat)
```
...

```{r vec2, echo=FALSE}
tail(mat, 4)
```
***

* After "preprocessing" the text, we can obtain two _long_ vectors. One with all the words and the other with the relevant document.

Output structure
========================================================
left: 65%
incremental: false

```{r vec3, echo=FALSE}
data(APtopics, package="LDAviz")
mat2 <- data.frame(mat, topic=APtopics$topics)
head(mat2)
```

...

```{r vec4, echo=FALSE}
tail(mat2, 4)
```
***

* After every iteration of gibbs sampling we will have a topic ID for each word. 
* Since memory can be an issue, we usually just save the topics from the last iteration (after convergence, of course).

What now?
========================================================

* From here, we construct point estimates of $\Phi$ and $\Theta$.
* In our "simple" example, $\Phi$ is `r length(APinput$vocab)` by `r max(mat2$topic)` and $\Theta$ is `r max(mat2$topic)` by `r max(mat2$doc_id)`
* __Problem__: Each topic has a unique pmf over `r length(APinput$vocab)` points!!! How do we interpret topics given such a large space?
* __Solution__: Rank words based on their "relevance" to each topic

What's relevance?
========================================================

- For a given topic $t$, rank words by:
  1. $p(w_i|t)$: common approach, but over-ranks common words
  2. $lift = p(w_i|t)/p(w_i)$: noisy and over-ranks less common words
  3. $relevance = \lambda*log(p(w_i|t)) + (1-\lambda)*log(lift)$ where $0<\lambda<1$
  4. A value of $\lambda = 1/3$ seems to work well

Global keywords
========================================================

1. $distinctiveness(w_i) = \sum_t p(t|w_i) log[\frac{p(t|w_i)}{p(t)}]$
<img src='upgrade.png' style="display:inline-block; margin-left:auto; margin-right:auto; height:205px;"/>
<img src='customer.png' style="display:inline-block; margin-left:auto; margin-right:auto; height:205px;"/>
2. $saliency(w_i) = p(w_i) * distinctiveness(w_i)$

Termite: Chuang, Heer & Manning '12
========================================================
incremental: false
type: prompt

<div align="center"><img src="termite.png" width=1200 height=800/></div>

Topic Similarities
========================================================

* Remember each topic has a unique distribution over words.
* We can calculate a dissimilarity between two topics by measuring the dissimilarity of their distributions (via a measure such as symmetric KL divergence)
* Computing dissimilarity for each unique pair of topics results in a high dimensional distance matrix.
* Thus, we use MDS to scale the points down to two dimensions (in order to visualize).
<li class="fragment" data-fragment-index="5">
<div align="center"><img src="scatter.png" width=400 height=300/></div>
<li>

Live Demo
========================================================
type: prompt

* These ideas (and some others) are integrated into a [fully interactive web application](http://glimmer.rstudio.com/cpsievert/LDAviz).
* This instance makes use of: 
  * The R package shiny
  * The JavaScript library D3
* By integrating these tools we can harness the best of both worlds!
  * R for statistical algorithms
  * JavaScript for interacting with graphics

```{r eval=FALSE, tidy=FALSE}
library(shiny) #run application from R
runGitHub("LDAviz", "kshirley", subdir="inst/shiny/hover")
```

Acknowledgements
========================================================

- Thanks to Kenny Shirley for being a great mentor.
- Thanks to Carlos Scheidegger for sharing his love & hatred for JavaScript.
- Thanks to all the members of Statistics Research for helpful feedback and discussions.


Questions/Comments/Compliments?
========================================================

<div align="center"><img src="awesome.jpeg" width=800 height=600/></div>